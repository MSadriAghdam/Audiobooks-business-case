{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "otherwise-buddy",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "junior-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-neighborhood",
   "metadata": {},
   "source": [
    "## Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statutory-projector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14084, 12)\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.loadtxt('Audiobooks_data.csv',delimiter = ',')\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "potential-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs = raw_data[:,1:-1]\n",
    "all_targets = raw_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-original",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "verbal-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see howmany 0s and 1s do we have in the targets\n",
    "ones_targets = [lambda x: x for x in all_targets if x==1]\n",
    "zeros_targets = [lambda x: x for x in all_targets if x==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confirmed-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of targets is:  14084\n",
      "The number of ones is:  2237\n",
      "The number of zeros is:  11847\n",
      "The percentage of ones/zeros:  18.88\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of targets is: \",len(all_targets))\n",
    "print(\"The number of ones is: \",len(ones_targets))\n",
    "print(\"The number of zeros is: \",len(zeros_targets))\n",
    "print(\"The percentage of ones/zeros: \",round(len(ones_targets)/len(zeros_targets)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "varying-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = []\n",
    "counter = 0\n",
    "\n",
    "for item  in range(len(all_targets)):\n",
    "    if all_targets[item] == 0:\n",
    "        counter +=1\n",
    "        if counter > len(ones_targets):\n",
    "            remove_list.append(item)\n",
    "    \n",
    "    \n",
    "balanced_data =np.delete(unscaled_inputs,remove_list, axis = 0)\n",
    "targets_equal_priors = np.delete(all_targets, remove_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-allocation",
   "metadata": {},
   "source": [
    "## Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comfortable-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-fifth",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "endless-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_ind = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arctic-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_inputs = scaled_inputs[shuffled_ind]\n",
    "shuffled_targets = targets_equal_priors[shuffled_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-general",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "laden-berkeley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579\n",
      "447\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "num_train_sample =int( 0.8 * len(shuffled_inputs))\n",
    "num_validation_sample =int( 0.1 * len(shuffled_inputs))\n",
    "num_test_sample = int( 0.1 * len(shuffled_inputs))\n",
    "print(num_train_sample)\n",
    "print(num_validation_sample)\n",
    "print(num_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "induced-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = shuffled_inputs[:num_train_sample]\n",
    "train_output =shuffled_targets[:num_train_sample]\n",
    "#validation samples starts from the end of a =training inputs a ends in a + number of validation samples \n",
    "validation_input = shuffled_inputs[num_train_sample:num_train_sample+num_validation_sample ]\n",
    "validation_output = shuffled_targets[num_train_sample:num_train_sample+num_validation_sample ]\n",
    "\n",
    "test_input = shuffled_inputs[num_train_sample+num_validation_sample:]\n",
    "test_output = shuffled_targets[num_train_sample+num_validation_sample:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "activated-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579 3579\n",
      "447 447\n",
      "448 448\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input), len(train_output))\n",
    "print(len(validation_input), len(validation_output))\n",
    "print(len(test_input), len(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faced-buffer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49399273540095\n",
      "0.5100671140939598\n",
      "0.5379464285714286\n"
     ]
    }
   ],
   "source": [
    "#Lets see how many 0s and 1s do we have in each dataset\n",
    "print(np.sum(train_output)/len(train_input))\n",
    "print(np.sum(validation_output)/len(validation_output))\n",
    "print(np.sum(test_output)/ len(test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-christopher",
   "metadata": {},
   "source": [
    "#### The result shows that we have got three balanced samples to train validate and test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "harmful-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_train',inputs = train_input, targets = train_output )\n",
    "np.savez('Audiobooks_validation',inputs = validation_input, targets = validation_output )\n",
    "np.savez('Audiobooks_test',inputs = test_input, targets = test_output )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-exclusion",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "drawn-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_train = np.load('Audiobooks_train.npz')\n",
    "train_inputs = npz_train['inputs'].astype(np.float)\n",
    "train_targets = npz_train['targets'].astype(np.int)\n",
    "\n",
    "npz_validation = np.load('Audiobooks_validation.npz')\n",
    "validation_inputs = npz_validation['inputs'].astype(np.float)\n",
    "validation_targets = npz_validation['targets'].astype(np.int)\n",
    "\n",
    "npz_test = np.load('Audiobooks_test.npz')\n",
    "test_inputs = npz_test['inputs'].astype(np.float)\n",
    "test_targets = npz_test['targets'].astype(np.int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-oklahoma",
   "metadata": {},
   "source": [
    "## Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "going-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2  # 0,1\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'), #the first hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'), #the second hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation = 'softmax') # the output is a calssifier\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "italian-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 2s - loss: 0.5342 - accuracy: 0.7332 - val_loss: 0.4393 - val_accuracy: 0.7494\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4171 - accuracy: 0.7835 - val_loss: 0.3823 - val_accuracy: 0.7987\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3843 - accuracy: 0.7946 - val_loss: 0.3532 - val_accuracy: 0.7987\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3644 - accuracy: 0.8027 - val_loss: 0.3434 - val_accuracy: 0.8121\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3591 - accuracy: 0.8030 - val_loss: 0.3366 - val_accuracy: 0.8054\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3497 - accuracy: 0.8114 - val_loss: 0.3305 - val_accuracy: 0.8233\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3475 - accuracy: 0.8019 - val_loss: 0.3324 - val_accuracy: 0.8098\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3437 - accuracy: 0.8125 - val_loss: 0.3288 - val_accuracy: 0.8076\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3374 - accuracy: 0.8192 - val_loss: 0.3185 - val_accuracy: 0.8255\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3334 - accuracy: 0.8215 - val_loss: 0.3158 - val_accuracy: 0.8345\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3321 - accuracy: 0.8217 - val_loss: 0.3192 - val_accuracy: 0.8166\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.3333 - accuracy: 0.8187 - val_loss: 0.3270 - val_accuracy: 0.8076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c87dd1af0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience =2)\n",
    "\n",
    "model.fit(\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    batch_size = batch_size ,\n",
    "    callbacks = [early_stop],\n",
    "    epochs = max_epochs,\n",
    "    validation_data = (validation_inputs, validation_targets),\n",
    "    verbose = 2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-kernel",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excessive-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8147\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "discrete-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.36. Test accuracy: 81.47%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-episode",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
